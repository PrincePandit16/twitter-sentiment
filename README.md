# ğŸ¦ Twitter Sentiment Analysis App

A Machine Learning web application that predicts whether a given tweet is **Positive** or **Negative**. Built with Python, Scikit-Learn, and Streamlit.

## ğŸ“‹ About The Project
This project uses Natural Language Processing (NLP) to analyze the sentiment of text data. It was trained on the **Sentiment140** dataset, which contains 1.6 million tweets extracted using the Twitter API.

The application takes a user's input (a tweet), pre-processes the text (cleaning, stemming, stopword removal), and runs it through a trained Logistic Regression model to classify the sentiment.

## ğŸ“Š Dataset
The model was trained on the **Sentiment140 dataset from Kaggle**.
* **Source:** [Sentiment140 on Kaggle](https://www.kaggle.com/datasets/kazanova/sentiment140)
* **Data Size:** 1.6 million tweets
* **Labels:** 0 (Negative), 4 (Positive) -> Mapped to 0 and 1.

## ğŸ› ï¸ Technologies Used
* **Python** (Programming Language)
* **Streamlit** (Web Framework)
* **Scikit-Learn** (Machine Learning Library)
* **Pandas & NumPy** (Data Manipulation)
* **NLTK** (Natural Language Toolkit for Stopwords/Stemming)
* **Pickle** (Model Serialization)

## ğŸ“‚ Project Structure
```text
twitter-sentiment-app/
â”‚
â”œâ”€â”€ app.py               # The main Streamlit web application
â”œâ”€â”€ requirements.txt     # List of dependencies
â”œâ”€â”€ trained_model.sav    # The trained Logistic Regression model
â”œâ”€â”€ vectorizer.sav       # The fitted TF-IDF Vectorizer
â”œâ”€â”€ Twitter.ipynb        # (Optional) The Jupyter Notebook used for training
â””â”€â”€ README.md            # Project documentation
```


## ğŸš€How to Run Locally

### 1. Clone the Repository
```bash
git clone [https://github.com/PrincePandit16/twitter-sentiment.git](https://github.com/PrincePandit16/twitter-sentiment.git)
cd twitter-sentiment
```
### 2. Install Dependencies
Make sure you have Python installed. It is recommended to use a virtual environment.
```bash
pip install -r requirements.txt
```

### 3.Run the app
```bash
streamlit run app.py
```
## ğŸ§  Model Details

### Preprocessing Pipeline
Before the text is fed into the model, the following steps are applied:
* **Regex Cleaning:** Removes non-alphabetic characters (punctuations, numbers, emojis).
* **Lowercasing:** Converts all text to lowercase for consistency.
* **Porter Stemming:** Reduces words to their root form (e.g., "running" -> "run").
* **Stopword Removal:** Removes common words (like "is", "the", "and") that don't carry sentiment.

### Feature Extraction
* **TF-IDF Vectorizer:** Converts text into numerical vectors (maximum 500,000 features).

### Algorithm
* **Logistic Regression:** Chosen for its efficiency and strong performance on binary text classification tasks.
* **Accuracy:** ~77.6% on Test Data.



## ğŸ¤ Contributing
Contributions, issues, and feature requests are welcome!

## ğŸ“œ License
This project is licensed under the [MIT License](https://choosealicense.com/licenses/mit/).

## ğŸ‘¤ Author
**PrincePandit16**
* GitHub: [@PrincePandit16](https://github.com/PrincePandit16)
